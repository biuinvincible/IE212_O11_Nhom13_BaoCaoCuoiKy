{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606aab58-abff-433c-bafc-41ee3562ac62",
   "metadata": {},
   "source": [
    "# Real-time Weather Prediction System \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0e6cf-dfb0-4cbc-9c3c-e5eb10a2a465",
   "metadata": {},
   "source": [
    "### Thành viên nhóm: \n",
    "##### 21522232 - Tạ Anh Khoa\n",
    "##### 21521214 - Đỗ Trọng Nhân\n",
    "##### 18521440 - Nguyễn Đức Thịnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759176fc-eeaa-47f5-889b-1043e72fe041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-0J6HSUP4:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Chronos</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dfa016a4c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark  \n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.functions import col\n",
    "import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.conf import SparkConf\n",
    "from sodapy import Socrata\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.0'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.6.1'\n",
    "]\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Chronos\") \\\n",
    ".master(\"local\") \\\n",
    ".config(\"spark.executor.memory\", \"16g\") \\\n",
    ".config(\"spark.driver.memory\", \"16g\") \\\n",
    ".config(\"spark.python.worker.reuse\", \"true\") \\\n",
    ".config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    ".config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"16\") \\\n",
    ".config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "conf=SparkConf()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a45172-9192-4f6c-bdea-ffe8f6339fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các modules cần thiết\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from bigdl.chronos.forecaster.tcn_forecaster import TCNForecaster\n",
    "from bigdl.chronos.metric.forecast_metrics import Evaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from bigdl.chronos.data import TSDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, StringIndexerModel, IndexToString, PolynomialExpansion\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30eeef99-9225-4b9c-96e9-46270717e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tập train\n",
    "train = spark.read.csv('Weather_train.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba76a28-146c-4ec9-9180-5dff110f9b86",
   "metadata": {},
   "source": [
    "### Tiền xử lí dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04744d34-202f-4df2-8bad-d09ef1d7637c",
   "metadata": {},
   "source": [
    "##### Đổi tên cột của tập train sao cho giống tập test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5068da0-6dfd-4161-b924-6c6cc5e44ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumnRenamed('Air Temperature', 'air_temperature')\n",
    "train = train.withColumnRenamed('Humidity', 'humidity')\n",
    "train = train.withColumnRenamed('Rain Intensity', 'rain_intensity')\n",
    "train = train.withColumnRenamed('Interval Rain', 'interval_rain')\n",
    "train = train.withColumnRenamed('Total Rain', 'total_rain')\n",
    "train = train.withColumnRenamed('Barometric Pressure', 'barometric_pressure')\n",
    "train = train.withColumnRenamed('Measurement Timestamp Label', 'measurement_timestamp_label')\n",
    "train = train.withColumnRenamed('Measurement Timestamp', 'measurement_timestamp')\n",
    "train = train.withColumnRenamed('Measurement Timestamp Label', 'measurement_timestamp_label')\n",
    "train = train.withColumnRenamed('Station Name', 'station_name')\n",
    "train = train.withColumnRenamed('Wet Bulb Temperature', 'wet_bulb_temperature')\n",
    "train = train.withColumnRenamed('Precipitation Type', 'precipitation_type')\n",
    "train = train.withColumnRenamed('Wind Direction', 'wind_direction')\n",
    "train = train.withColumnRenamed('Wind Speed', 'wind_speed')\n",
    "train = train.withColumnRenamed('Maximum Wind Speed', 'maximum_wind_speed')\n",
    "train = train.withColumnRenamed('Solar Radiation', 'solar_radiation')\n",
    "train = train.withColumnRenamed('Heading', 'heading')\n",
    "train = train.withColumnRenamed('Battery Life', 'battery_life')\n",
    "train = train.withColumnRenamed('Measurement ID', 'measurement_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8bdbd-cbd6-4e98-8d50-5787e7925236",
   "metadata": {},
   "source": [
    "##### Hàm thêm một cột để chuyển đổi giờ trong bộ dữ liệu sang giờ quốc tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b3971f-cb1e-403b-a25a-14b6410c9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twelve_to_twentyfour(Hour,AorP):\n",
    "    hour = ''\n",
    "    if (AorP == 'AM'):\n",
    "        if ((int)(Hour) < 10):\n",
    "            hour = \"0\"+(Hour)\n",
    "        elif (Hour == '12'):\n",
    "            hour = '00'\n",
    "        else: \n",
    "            hour = Hour\n",
    "    else:\n",
    "        if (Hour == '12'):\n",
    "            hour = '12'\n",
    "        else:\n",
    "            hour = ((int)(Hour) + 12)\n",
    "    return hour\n",
    "Hour_Converter = udf(twelve_to_twentyfour, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6d6c0-f836-47eb-9222-e6802e54f3a4",
   "metadata": {},
   "source": [
    "##### Hàm trả về chuỗi thời gian đã được xử lí với giờ đã được chuyển thành giờ quốc tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab34a0fc-50d4-4464-81b7-3873962052f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_merger(date, month, year, hour):\n",
    "    return (year) + \"-\" + (month) + \"-\" +(date)+ \" \" + (hour)+\":00:00\"\n",
    "timestamp_merger_udf = udf(timestamp_merger, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e698d-7c10-40ad-afac-8036f761c82a",
   "metadata": {},
   "source": [
    "##### Hàm trả về một dataframe mới đã được thêm cột timestamp và đã được sắp xếp tăng dần theo cột timestamp vừa thêm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359d2d63-1bd3-445c-a065-daef295c27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_adder(df):\n",
    "    copy = df.alias('copy')\n",
    "    copy = copy.withColumn('Date',f.split(col('measurement_timestamp_label'),' ')[0])\n",
    "    copy = copy.withColumn('Hour',(f.split(col('measurement_timestamp_label'),' ')[1]))\n",
    "    copy = copy.withColumn('AMorPM',(f.split(col('measurement_timestamp_label'),' ')[2]))                         \n",
    "    copy = copy.withColumn('Month',f.split(col('Date'),'/')[0].cast('string'))\n",
    "    copy = copy.withColumn('Day',f.split(col('Date'),'/')[1].cast('string'))\n",
    "    copy = copy.withColumn('Year',f.split(col('Date'),'/')[2].cast('string'))\n",
    "    copy = copy.withColumn('Time', Hour_Converter(f.split(copy.Hour,':')[0], copy.AMorPM).cast('string'))\n",
    "    copy = copy.withColumn('timestamp',timestamp_merger_udf(copy.Day,copy.Month,copy.Year,copy.Time))\n",
    "    copy = copy.withColumn('timestamp', f.to_timestamp('timestamp', 'yyyy-MM-dd HH:mm:ss'))\n",
    "    copy = copy.orderBy('timestamp')\n",
    "    copy = copy.dropna()\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e93f16-df3d-45b1-94dc-62c02cbfe7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = timestamp_adder(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8d648-8b1a-47ca-a2be-3c1ef594be50",
   "metadata": {},
   "source": [
    "#### Xử lí dữ liệu bị âm trong biến interval_rain và rain_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be56ecd9-8f35-40aa-8db1-621c2fc487d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_outlier(x):\n",
    "    return abs(x)\n",
    "interval_outliers_udf = udf(interval_outlier, DoubleType())\n",
    "train = train.withColumn('interval_rain', interval_outliers_udf(train.interval_rain))\n",
    "def intensity_outlier(x):\n",
    "    return abs(x)\n",
    "intensity_outliers_udf = udf(intensity_outlier, DoubleType())\n",
    "train = train.withColumn('rain_intensity', intensity_outliers_udf(train.rain_intensity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd39ba1-69ed-400c-b554-472b38a5ec27",
   "metadata": {},
   "source": [
    "##### Chỉ lấy dữ liệu từ 15 giờ ngày 17/05/2021 trở về sau vì dữ liệu trong khoảng này ổn định và đảm bảo tính liên tục."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e9ce37-9fc2-4b09-899c-37e82f76138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.filter(col('timestamp') >= \"2021-05-17 15:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2ba1a-2946-4612-9ab0-3d4ef2b497ac",
   "metadata": {},
   "source": [
    "##### List chứa các biến mục tiêu để mô hình TCNForecaster dự đoán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c944c46c-e92b-4b31-9ee0-a35fcdabef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['air_temperature','interval_rain','humidity','barometric_pressure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5477bd-c3cf-495d-ae07-42d6f4997094",
   "metadata": {},
   "source": [
    "##### List chứa các biến độc lập sẽ được coi như là extra_feature_col khi tạo dữ liệu với TSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e336569d-231c-442e-9f43-2751a3101ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_feature_cols = ['rain_intensity','total_rain']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78528d9a-11d1-42d7-ad45-0da5a8939134",
   "metadata": {},
   "source": [
    "### Huấn luyện mô hình TCNForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abd82eb3-9113-4dc8-96d0-4f29c8016eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "WARNING:root:There are irregular interval(more than one interval length) among the data. You can call .resample(interval).impute() first to clean the data manually, or set repair=True when initialize TSDataset.\n",
      "WARNING:root:Dataframe has be resampled according to interval 0 days 01:00:00.\n",
      "WARNING:root:The missing value of column air_temperature exceeds 0,please call .impute() fisrt to remove N/A number manually, or set repair=True when initialize TSDataset.\n",
      "WARNING:root:Missing data has be imputed.\n",
      "WARNING:root:Some values of column rain_intensity exceeds the mean plus/minus 10 times standard deviation, please call .repair_abnormal_data() to remove abnormal values.\n",
      "WARNING:root:There are irregular interval(more than one interval length) among the data. You can call .resample(interval).impute() first to clean the data manually, or set repair=True when initialize TSDataset.\n",
      "WARNING:root:Dataframe has be resampled according to interval 0 days 01:00:00.\n",
      "WARNING:root:The missing value of column air_temperature exceeds 0,please call .impute() fisrt to remove N/A number manually, or set repair=True when initialize TSDataset.\n",
      "WARNING:root:Missing data has be imputed.\n",
      "WARNING:root:Some values of column rain_intensity exceeds the mean plus/minus 10 times standard deviation, please call .repair_abnormal_data() to remove abnormal values.\n"
     ]
    }
   ],
   "source": [
    "tsdata_train, _, tsdata_test = TSDataset.from_pandas(train.toPandas(), dt_col=\"timestamp\", target_col=target_cols,extra_feature_col=extra_feature_cols,repair=True,\n",
    "                                                                with_split=True, test_ratio=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362f54f-f24b-4c0a-9aa3-f53633730caf",
   "metadata": {},
   "source": [
    "##### Set lookback = 12 (12 giờ trước) và dự đoán cho 1 giờ sau (horizon = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3b42563-1457-41c5-9552-e92696867261",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback, horizon = 12, 1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "for tsdata in [tsdata_train, tsdata_test]:\n",
    "    tsdata.deduplicate()\\\n",
    "          .impute()\\\n",
    "          .gen_dt_feature()\\\n",
    "          .scale(scaler, fit=(tsdata is tsdata_train))\\\n",
    "          .roll(lookback=lookback, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef8e9c87-0f07-405d-88cf-54d04be4b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | NormalizeTSModel | 134 K \n",
      "1 | loss  | MSELoss          | 0     \n",
      "-------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.536     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a631edd43442445a81fb747096957d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = tsdata_train.to_numpy()\n",
    "forecaster = TCNForecaster(past_seq_len=lookback,  \n",
    "                           future_seq_len=horizon,  \n",
    "                           input_feature_num=x.shape[-1],\n",
    "                           output_feature_num=y.shape[-1],\n",
    "                           num_channels=[64,72,72],\n",
    "                           kernel_size=5,\n",
    "                           dropout=0,\n",
    "                           seed=0)\n",
    "forecaster.fit((x, y), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "348ee92b-3672-4c68-affd-24b7e1088744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE is 25.5754602156829\n",
      "mean_squared error is 4.730735919024346\n",
      "r2 is 0.9999732260186943\n"
     ]
    }
   ],
   "source": [
    "# Predict with test set and perform some evaluations\n",
    "x_test, y_test = tsdata_test.to_numpy()\n",
    "pred = forecaster.predict(x_test)\n",
    "pred_unscale, groundtruth_unscale = tsdata_test.unscale_numpy(pred), tsdata_test.unscale_numpy(y_test)\n",
    "pred_unscale = pred_unscale.reshape(pred_unscale.shape[0], -1)\n",
    "groundtruth_unscale = groundtruth_unscale.reshape(groundtruth_unscale.shape[0],-1)\n",
    "res = Evaluator.evaluate([\"smape\", \"mse\",\"r2\"],\n",
    "                         y_true=groundtruth_unscale,\n",
    "                         y_pred=pred_unscale)\n",
    "# evaluate with sMAPE\n",
    "print(\"sMAPE is\", res[0])\n",
    "# evaluate with mean_squared_error\n",
    "print(\"mean_squared error is\", res[1])\n",
    "# evaluate with r2\n",
    "print(\"r2 is\", res[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2625c5-62fc-4b90-b08b-57ed4150da43",
   "metadata": {},
   "source": [
    "### Tạo một dataframe chứa các giá trị vừa dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d638dcda-6ca3-4ef8-b1ee-e54741b15aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"air_temperature\", DoubleType(), True),\n",
    "    StructField(\"interval_rain\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),\n",
    "    StructField(\"barometric_pressure\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24b99d69-53d1-48ef-b430-7f89feb38375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "predict = spark.createDataFrame(pred_unscale, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d423e2b6-76dd-424d-af6b-2046730aaaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rain_interval_standarder(interval):\n",
    "    if (interval < 0.1):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return interval\n",
    "rain_interval_standarder_udf = udf(rain_interval_standarder, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3911b9d5-7f15-4483-86ea-a8f444cf387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chuẩn hóa giá trị các biến\n",
    "def vars_standarder(df):\n",
    "    copy = df.alias('copy')\n",
    "    copy = copy.withColumn('interval_rain', rain_interval_standarder_udf(df.interval_rain))\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbd1d2e5-e93d-4394-8264-11345e25c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trả về một dataframe đã được định nghĩa sẵn schema giống với train dataframe.\n",
    "def cast_dataframe(df):\n",
    "    # Define the schema\n",
    "    schema = StructType([\n",
    "        StructField(\"station_name\", StringType(), True),\n",
    "        StructField(\"measurement_timestamp\", StringType(), True),\n",
    "        StructField(\"air_temperature\", DoubleType(), False),\n",
    "        StructField(\"wet_bulb_temperature\", DoubleType(), True),\n",
    "        StructField(\"humidity\", IntegerType(), True),\n",
    "        StructField(\"rain_intensity\", DoubleType(), True),\n",
    "        StructField(\"interval_rain\", DoubleType(), True),\n",
    "        StructField(\"total_rain\", DoubleType(), True),\n",
    "        StructField(\"precipitation_type\", IntegerType(), True),\n",
    "        StructField(\"wind_direction\", IntegerType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"maximum_wind_speed\", DoubleType(), True),\n",
    "        StructField(\"barometric_pressure\", DoubleType(), False),\n",
    "        StructField(\"solar_radiation\", IntegerType(), True),\n",
    "        StructField(\"heading\", IntegerType(), True),\n",
    "        StructField(\"battery_life\", DoubleType(), True),\n",
    "        StructField(\"measurement_timestamp_label\", StringType(), True),\n",
    "        StructField(\"measurement_id\", StringType(), True),\n",
    "        StructField(\"Date\", StringType(), True),\n",
    "        StructField(\"Hour\", StringType(), True),\n",
    "        StructField(\"AMorPM\", StringType(), True),\n",
    "        StructField(\"Month\", StringType(), True),\n",
    "        StructField(\"Day\", StringType(), True),\n",
    "        StructField(\"Year\", StringType(), True),\n",
    "        StructField(\"Time\", StringType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True)\n",
    "    ])\n",
    "    # Cast each column to the correct type\n",
    "    for field in schema.fields:\n",
    "        df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42f25ace-1791-407f-9647-fe855a88ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = vars_standarder(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dee6c599-8304-456f-88f4-f655445bc161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+------------------+-------------------+\n",
      "|    air_temperature|interval_rain|          humidity|barometric_pressure|\n",
      "+-------------------+-------------+------------------+-------------------+\n",
      "| -4.964935302734375|          0.0|  63.6673583984375|  998.8425903320312|\n",
      "| -4.658403396606445|          0.0|  63.4568977355957|  999.4796142578125|\n",
      "| -3.322305679321289|          0.0| 62.15800094604492|  999.6537475585938|\n",
      "|-1.2513647079467773|          0.0|57.960140228271484|  999.7672729492188|\n",
      "|-0.6242895126342773|          0.0| 58.21668243408203|  999.8861083984375|\n",
      "| 0.2501564025878906|          0.0| 58.17109680175781|      998.701171875|\n",
      "| 1.0307893753051758|          0.0|56.286407470703125|   997.632080078125|\n",
      "| 1.4825973510742188|          0.0| 57.76877975463867|    996.73193359375|\n",
      "| 1.1421918869018555|          0.0| 60.03619384765625|  996.7979125976562|\n",
      "| 1.0093050003051758|          0.0| 62.15092849731445|  996.7252807617188|\n",
      "+-------------------+-------------+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2e752-c2b2-43f8-9cf0-2447e21e54a6",
   "metadata": {},
   "source": [
    "### Train một mô hình RandomForestClassifier để phân loại loại mưa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a48a2f0-6f66-4190-b150-b70209ac61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_Indexer = StringIndexer(inputCol='precipitation_type', outputCol='label', handleInvalid='keep')\n",
    "assembler = VectorAssembler(inputCols=target_cols,outputCol='features')\n",
    "poly = PolynomialExpansion(degree=2,inputCol='features',outputCol='poly_features')\n",
    "scale = pyspark.ml.feature.StandardScaler(inputCol='poly_features',outputCol='scaled_features')\n",
    "rf = RandomForestClassifier(featuresCol='scaled_features',labelCol='label',numTrees=100,maxDepth=10)\n",
    "pipeline = Pipeline(stages=[label_Indexer,assembler,poly,scale,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64dd337d-40a8-4455-b253-84fc527025e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e7f78ad-fe88-4fa5-84a4-2724180236a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f0a89ba-acfb-439a-b0a2-c83ee399c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_int_to_string(pred):\n",
    "    if pred == 0:\n",
    "        return \"Không mưa\"\n",
    "    elif pred == 1:\n",
    "        return \"Mưa dạng lỏng\"\n",
    "    elif pred == 2:\n",
    "        return \"Mưa dạng rắn\"\n",
    "    else:\n",
    "        return \"Loại mưa không xác định\"\n",
    "    return \"\"\n",
    "prediction_to_string = udf(prediction_int_to_string, StringType())\n",
    "prediction = prediction.withColumn('rain_type',prediction_to_string(prediction.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb31caf-3faf-4e87-991a-edc33599b81d",
   "metadata": {},
   "source": [
    "### Khai báo streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0208e371-5942-4ddd-bff9-47588dd62a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'TestDoAn'\n",
    "kafka_server = 'localhost:9092'\n",
    "streamRawDf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).option(\"startingOffsets\",\"latest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32104f24-9c0a-4e39-a8e1-3b7c25d6d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers= ['station_name','measurement_timestamp','air_temperature','wet_bulb_temperature','humidity','rain_intensity','interval_rain','total_rain','precipitation_type','wind_direction','wind_speed','maximum_wind_speed','barometric_pressure','solar_radiation','heading','battery_life','measurement_timestamp_label','measurement_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23672e52-5808-4a9f-92e3-15ff977a97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = streamRawDf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stream_writer = (df.writeStream.queryName('Item').trigger(processingTime=\"5 seconds\").outputMode(\"append\").format(\"memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d06d694-4300-447c-bc7e-4f07f94116be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b416709-85a3-4cad-bf57-62e78356b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_Client = MongoClient('localhost:27017')\n",
    "collection = mongo_Client.weather.weather_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e412b146-bf16-4ad2-9650-0df9d7c90806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_number_1(number):\n",
    "    return round(number,1)\n",
    "def round_number_2(number):\n",
    "    return round(number,2)\n",
    "def round_int_number(number):\n",
    "    return ((int)(number))\n",
    "round_udf_1 = udf(round_number_1, DoubleType())\n",
    "round_udf_2 = udf(round_number_2, DoubleType())\n",
    "round_int = udf(round_int_number, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "359c3831-7b41-47ec-9b1f-9df0ca42a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_rounder(df):\n",
    "    copy = df.alias('copy')\n",
    "    copy = copy.withColumn('interval_rain', round_udf_1(copy.interval_rain))\n",
    "    copy = copy.withColumn('barometric_pressure', round_int(copy.barometric_pressure))\n",
    "    copy = copy.withColumn('air_temperature', round_udf_2(copy.air_temperature))\n",
    "    copy = copy.withColumn('humidity', round_int(copy.humidity))\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd41b2-1e99-41c8-958a-b64c05e7b792",
   "metadata": {},
   "source": [
    "### Dự đoán real-time sau đó lưu vào MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeaa677-cd28-450c-a98d-71f90b04bf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing live view refreshed every 5 seconds\n",
      "Seconds passed: 310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "D:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "time = 0;\n",
    "while (True):\n",
    "    try:\n",
    "        print('Showing live view refreshed every 5 seconds')\n",
    "        print(f'Seconds passed: {time*5}')\n",
    "        result = spark.sql(f\"SELECT distinct value from {query.name}\")\n",
    "        result = result.select(f.regexp_replace(\"value\", \"[^0-9a-zA-Z_,.:/ \\-]+\", \"\").alias('replaced_st'))\n",
    "        for i in range(0,len(headers)):\n",
    "            result = result.withColumn(headers[i],f.split(col('replaced_st'),\",\")[i])\n",
    "        result = result.drop('replaced_st')\n",
    "        result = timestamp_adder(result)\n",
    "        result = cast_dataframe(result)\n",
    "        result = result.select(train.columns)\n",
    "        # Chỉ khi nào số records lớn hơn hoặc bằng lookbback + 1 thì bắt đầu dự đoán cho 1 giờ sau.\n",
    "        if ((result.count() > lookback)):\n",
    "            # Tạo một dataframe chứa (lookback + 1) giá trị mới nhất từ nguồn dữ liệu.\n",
    "            result = spark.createDataFrame(result.tail(13))\n",
    "            result = result.withColumn('interval_rain', interval_outliers_udf(result.interval_rain))\n",
    "            result = result.withColumn('rain_intensity', intensity_outliers_udf(result.rain_intensity))\n",
    "            train_data = TSDataset.from_pandas(result.toPandas(), dt_col=\"timestamp\", target_col=target_cols,extra_feature_col=extra_feature_cols,with_split=False)\n",
    "            train_data.deduplicate().impute().gen_dt_feature().scale(scaler, fit=True).roll(lookback=lookback, horizon=horizon)\n",
    "            X,Y = train_data.to_numpy()\n",
    "            pre = forecaster.predict(X)\n",
    "            # Unscale kết quả dự đoán\n",
    "            output = train_data.unscale_numpy(pre)\n",
    "            # Reshape tensor kết quả để dễ dàng tạo dataframe với nó\n",
    "            output = output.reshape(output.shape[0], -1)\n",
    "            # Tạo một dataframe từ kết quả dự đoán mới được reshape\n",
    "            result = spark.createDataFrame(output,schema)\n",
    "            result = vars_standarder(result)\n",
    "            result = output_rounder(result)\n",
    "            result = model.transform(result).select('air_temperature','interval_rain','humidity','barometric_pressure','prediction')\n",
    "            result = result.withColumn('rain_type',prediction_to_string(result.prediction))\n",
    "            collection.delete_many({})\n",
    "            collection.insert_one(result.tail(1)[0].asDict())\n",
    "            display(result.show(truncate=False))\n",
    "        sleep(5)\n",
    "        clear_output(wait=True)\n",
    "        time += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print('break')\n",
    "        break\n",
    "print('Live view ended..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9e44a-6a38-4b40-936b-68c927a53e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
